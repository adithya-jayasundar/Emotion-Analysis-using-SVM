# -*- coding: utf-8 -*-
"""Untitled55.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AsOvW2M9-tFz0O-_wMlQi7xJxZXqVZ4R
"""

import torch
from transformers import BertTokenizer, BertModel
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Load dataset
df = pd.read_csv('data_test.csv')  # Ensure this CSV has 'Text' and 'Emotion' columns
print(df.head())

# Step 2: Prepare text and label data
texts = df['Text'].tolist()
labels = df['Emotion'].tolist()

# Encode string labels into numeric format
emotion_map = {'sadness': 0, 'neutral': 1, 'anger': 2, 'fear': 3, 'joy': 4}
y = np.array([emotion_map[label] for label in labels])

# Step 3: Tokenize text using BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
encodings = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)

# Step 4: Load BERT model and set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertModel.from_pretrained('bert-base-uncased').to(device)
encodings = {key: value.to(device) for key, value in encodings.items()}

# Step 5: Extract [CLS] token embeddings from BERT
batch_size = 32
X = []

with torch.no_grad():
    for i in range(0, len(encodings['input_ids']), batch_size):
        batch = {key: value[i:i + batch_size] for key, value in encodings.items()}
        outputs = model(**batch)
        cls_embeddings = outputs.last_hidden_state[:, 0, :]
        X.extend(cls_embeddings.cpu().numpy())

X = np.array(X)

# Step 6: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Step 7: Train SVM classifier
clf = svm.SVC(kernel='linear', C=1.0, random_state=42)
clf.fit(X_train, y_train)

# Step 8: Evaluate classifier performance
y_pred = clf.predict(X_test)
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=emotion_map.keys()))
print("Accuracy:", accuracy_score(y_test, y_pred))

# Step 9: Plot confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=emotion_map.keys(),
            yticklabels=emotion_map.keys())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Step 10: Manual emotion prediction
def predict_emotion(texts_to_predict):
    model.eval()
    inputs = tokenizer(texts_to_predict, padding=True, truncation=True, return_tensors='pt', max_length=512)
    inputs = {key: value.to(device) for key, value in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        cls_embeddings = outputs.last_hidden_state[:, 0, :]
        cls_embeddings = cls_embeddings.cpu().numpy()

    predictions = clf.predict(cls_embeddings)
    inv_emotion_map = {v: k for k, v in emotion_map.items()}
    predicted_emotions = [inv_emotion_map[pred] for pred in predictions]

    for text, emotion in zip(texts_to_predict, predicted_emotions):
        print(f"Text: \"{text}\" → Predicted Emotion: {emotion}")

# Example usage
print("\n--- Manual Test Case ---")
sample_texts = [
    "I'm so happy with how today went!",
    "I feel really down right now.",
    "I'm not sure how to feel about this.",
    "Why would you say something like that?!",
    "I’m scared of what might happen tomorrow."
]
predict_emotion(sample_texts)

